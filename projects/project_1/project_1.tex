\documentclass[12pt]{article}
\usepackage[top=3cm, bottom=3cm, right=3cm, left=3cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}

\lstset{language=Matlab,
basicstyle=\scriptsize,
keywordstyle=\color{keywords},
commentstyle=\color{comments},
stringstyle=\color{blue},
showstringspaces=false,
tabsize=4,
identifierstyle=\color{green},
numberstyle=\tiny,
numbersep=5pt,
showstringspaces=false,
procnamekeys={def,class}}

\begin{document}
\title{FYS3150 - PROJECT 1 - AUTUMN 2015}
\author{Mari Dahl Eggen}
\maketitle

\newpage

\begin{flushleft}
\begin{abstract}
In this project we will develop an algorithm to solve a matrix equation where the matrix is tridiagonal. The goal is to develop an algorithm that has fewer floating point operations that the algorithms for Gaussian Elimination and LU-decomposition, so that the calculations are running faster, and so that we can do the calculations for bigger systems. Through different tests described in this report, we find that the customized algorithm was a success.
\end{abstract}
\section*{Introduction}
Some times it pays of to develop algorithms for special cases in the sciences. The reason for that is that the calculations on a computer can get very slow if the number of floating point operations is getting too big. In addition, the more floating point operations, the greater is the probability of round off errors in the calculations. If a customized algorithm is made, the floating point operations can be cut down to a minimum of what is needed, and then the algorithm can solve grater systems of the kind it is customized to solve. In this project an algorithm customized to solve matrix equations, where the matrix is tridiagonal, is developed. The algorithm is based on the method of Gaussian elimination, and we will through different tests show that the customized algorithm is both working as that method, and that it is faster. In addition we will find the optimal number of grid-point to execute the algorithm with, so that the relative error of the numerical solution is getting as small as possible.


\newpage
\section*{Theory}
\subsection*{Poisson equation}
Poisson's equation is a classical equation from electromagnetism, and with spherical symmetrical properties it reads\\

$$\frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d\Phi(r)}{dr}\right) = -4\pi \rho(r),$$\\
\vspace{5mm}

where $\Phi$ is a electrostatic potential generated by the localized charge distribution $\rho(\bf r).$ This equation can be expressed in a more simple way by use of the substitution $\Phi(r) = \frac{\phi(r)}{r}$ as\\

$$\frac{d^2\phi}{dr^2}= -4\pi r\rho(r).$$\\
\vspace{5mm}

A standard linear second-order differential equation reads\\

$$\frac{d^2u}{dx^2}+k^2(x)u = f(x),$$\\
\vspace{5mm}

where $f(x)$ is the called inhomogenous term and $k^{2}(x)$ is a real function. If we let $\phi \rightarrow u$ and $r\rightarrow x$, sets $k^{2}(x) = 0$,  and sets $f(x) = -4\pi\rho(x)$, the one-dimensional Poisson equation can be expressed as\\

\begin{equation}\label{eq:2_diff_eq}
-u''(x) = f(x).
\end{equation}
\vspace{5mm}

One way to solve Equation (\ref{eq:2_diff_eq}) is to rewrite it as a set of linear equations, where we define the discretized approximation of $u$ as $v_{i}$. The grid points are set to $x_{i} = ih$ in the interval $[x_{0},x_{n+1}]$. $u''(x)$ can then be approximated as\\

\begin{equation}\label{eq:double_derivative}
-\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dots, n,
\end{equation}\\
\vspace{5mm}
where $f_i=f(x_i)$. In such a problem we need to have some boundary conditions. One set of those is called Dirichlet boundary conditions, and reads\\

\begin{equation}\label{eq:boundary}
x\in (0,1),\quad u(0) = u(1) = 0.
\end{equation}
 

\subsection*{Gaussian elimination}
Gaussian elimination is a method of solving a linear set of equations, which can be represented as the matrix equation\\

\begin{equation}\label{eq:the_matrix_eq}
\bf A\bar{v} = \bar{b}
\end{equation}
$$\Rightarrow \quad
    \left(\begin{array}{cccc}
    a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
    a_{2,1} & \ddots &  &\vdots \\
    \vdots &  & \ddots & \vdots \\
    a_{n,1} & \dots & \dots & a_{n,n}\\
    \end{array} \right)    
    \left(\begin{array}{c}
    v_{1}\\
    \vdots\\
    \vdots\\
    v_{n}\\
    \end{array} \right) =
    \left(\begin{array}{c}
    b_{1}\\
    \vdots\\
    \vdots\\
    b_{n}\\
    \end{array} \right)
$$\\
\vspace{5mm}
$$ 
    \Rightarrow\quad
    \begin{array}{c}
    a_{1,1}v_{1} + a_{1,2}v_{2} + \dots + a_{1,n}v_{n} = b_{1}\\
    a_{2,1}v_{1} + a_{2,2}v_{2} + \dots + a_{2,n}v_{n} = b_{2}\\
    \vdots\\
    a_{n,1}v_{1} + a_{n,2}v_{2}\dots + a_{n,n}v_{n} = b_{n},\\    
    \end{array}
$$\\
\vspace{5mm}    
in the case of a quadratic nxn-matrix. We will look at the simplest case where we assume that the matrix $\bf A$ is non-singular, and that the matrix elements $a_{i,i} \neq 0$. In this method the aim is to eliminate the unknowns from the set of equations, in a systematical way, so that the set of equations takes the form\\

\begin{equation}\label{eq:elim_eq_set}
    \begin{array}{c}
    a_{1,1}v_{1} + a_{1,2}v_{2} + \dots + a_{1,n}v_{n} = b_{1}\\
    a_{2,2}v_{2} + a_{2,3}v_{3} + \dots + a_{2,n}v_{n} = b_{2}\\
    a_{3,3}v_{3} + a_{3,4}v_{4} + \dots + a_{3,n}v_{n} = b_{3}\\
    \vdots\\
    a_{n,n}v_{n} = b_{n}.\\    
    \end{array}
\end{equation}\\
\vspace{5mm}
One can reach that form by transforming the the desired coefficients in the matrix $\bf A$, one by one, to zero. The method for doing this is operations done on the set of equations, and is called forward substitution. The formula for the left hand side of the equations is\\

\begin{equation}\label{eq:lhs_forw}
a_{j,k}^{(m+1)} = a_{j,k}^{(m)}-\frac{a_{j,m}^{(m)}\cdot a_{m,k}^{(m)}}{a_{m,m}^{(m)}},\quad j,k = m+1,...,n\quad\text{and}\quad m = 1,...,n-1,
\end{equation}
\newpage
and the formula for the right hand side is\\
\vspace{5mm}
\begin{equation}\label{eq:rhs_forw}
b_{j}^{(m+1)} = b_{j}^{(m)}-\frac{a_{j,m}^{(m)}\cdot b_{m}^{(m)}}{a_{m,m}^{(m)}},\quad j = m+1,...,n\quad\text{and}\quad m = 1,...,n-1.
\end{equation}
\vspace{5mm}

The set of equations (\ref{eq:elim_eq_set}) is easy to solve. The method of doing it is called backward substitution, and the formula for doing it is\\
\vspace{5mm}
\begin{equation}\label{eq:backw}
v_{m} = \frac{1}{a_{m,m}}\left(b_{m} - \sum_{k=m+1}^{n}a_{m,k}\cdot v_{k}\right),\quad m = n-1,n-2,...,1.
\end{equation}
 \vspace{5mm}
\subsection*{LU-decomposition}
This method is a form of Gaussian elimination where the starting point is a factorization of the the matrix $\bf A$ in Equation (\ref{eq:the_matrix_eq}). $\bf A$ is factorized as two matrices called $L$ and $U$, where the names comes from the fact that the matrices are lower and upper diagonal. Thus we have\\

$$\bf A = \bf LU$$\\
$$\Rightarrow \quad
    \left(\begin{array}{cccc}
    a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
    a_{2,1} & \ddots &  &\vdots \\
    \vdots &  & \ddots & \vdots \\
    a_{n,1} & \dots & \dots & a_{n,n}\\
    \end{array} \right) =
    \left(\begin{array}{cccc}
    1 & 0 & \dots & 0 \\
    l_{2,1} & 1 &  &\vdots \\
    \vdots & \ddots & \ddots & 0 \\
    l_{n,1} & \dots & l_{n,n-1} & 1\\
    \end{array} \right) 
    \left(\begin{array}{cccc}
    u_{1,1} & u_{1,2} & \dots & u_{1,n} \\
    0 & u_{2,2} &  &\vdots \\
    \vdots & & \ddots & \vdots \\
    0 & \dots & 0 & u_{n,n}\\
    \end{array} \right). 
$$\\ 
\vspace{5mm}
The quadratic matrix $\bf A$ has a LU factorization if its determinant is different from zero, and the factorization is unique if $\bf A$ is non-singular. The LU factorization can be found by use of the two equations\\

$$i\leq j:\quad l_{i,1}u_{1,j} + l_{i,2}u_{2,j}+...+l_{i,i}u_{i,j} = a_{i,j}$$\\
$$i>j:\quad l_{i,1}u_{1,j} + l_{i,2}u_{2,j}+...+l_{i,j}u_{j,j} = a_{i,j}.$$\\
\newpage
Now the problem of solving Equation (\ref{eq:the_matrix_eq}) has evolved to be the problem of solving the equation\\

\begin{equation}\label{eq:LU_eq}
\bf LU \bar{v} = \bar{b}.
\end{equation}\\
\vspace{5mm}
It can be shown that Equation (\ref{eq:LU_eq}) can be split into two equations on the form\\
\begin{equation}\label{eq:LU_eq}
\bf L \bar{y} = \bar{b}\quad\quad U\bar{v} = \bar{y}.
\end{equation}\\
\vspace{5mm}
$$\Rightarrow\quad
    \begin{array}{c}
    y_{1}=b_{1}\\
    l_{2,1}y_{1} + y_{2} = b_{2}\\
    l_{3,1}y_{1} + l_{3,2}y_{2} + y_{3} = b_{3}\\
    \vdots\\
    l_{n,1}y_{1} + l_{n,2}y_{2} + \dots + l_{n,n}y_{n} = b_{n}\\       
    \end{array}
    \quad\quad
    \begin{array}{c}
    u_{1,1}v_{1} + u_{1,2}v_{2} + \dots + u_{1,n}v_{n} = y_{1}\\
    u_{2,2}v_{2} + u_{2,3}v_{3} + \dots + u_{2,n}v_{n} = y_{2}\\
    u_{3,3}v_{3} + u_{3,4}v_{4} + \dots + u_{3,n}v_{n} = y_{3}\\
    \vdots\\
    u_{n,n}v_{n} = y_{n}.\\    
    \end{array}
$$\\
 \vspace{5mm}
These equations can be solved by use of backward substitution, that was discussed in the previous section. First to find $\bf \bar{y}$, and then to find $\bf \bar{v}$.

\subsection*{Relative error}
The relative error of a computed data set $i = 1,...,n$ can be calculated by use of the formula\\

\begin{equation}\label{eq:relative_error}
\epsilon_{i} = log_{10}\left(\left|\frac{v_i - u_i}{u_i}\right|\right),
\end{equation}
\vspace{5mm}

where $v_i$ is the elements in the data set for the numerical solution and $u_i$ is the elements in the data set for the analytically solution.

\newpage

\section*{Method}
To show that Equation (\ref{eq:double_derivative}) can be written as a linear set of equations on the form\\

\begin{equation}\label{eq:matrix_eq}
{\bf A}{\bf \bar{v}} = {\bf \bar{b}},
\end{equation}\\

where ${\bf A}$ is the $n\times n$  tridiagonal matrix\\
 \vspace{5mm}
\begin{equation}\label{matrix:tridiag_deriv}
{\bf A} = 
    \left(\begin{array}{cccccc}
    2& -1& 0 &\dots   & \dots &0 \\
    -1 & 2 & -1 &0 &\dots &\vdots \\
     0&-1 &2 & -1 & 0 & \vdots \\
     \vdots & \dots   & \dots &\dots   &\dots & \vdots \\
     0&\dots   &\dots  &-1 &2& -1 \\
     0&\dots    &\dots  & 0  &-1 & 2 \\
    \end{array} \right)
\end{equation}\\
    
\vspace{5mm}

and $b_i=h^2f_i$, we will use Equation (\ref{eq:matrix_eq}) as the starting point.\\
\vspace{5mm}
$$\left(\begin{array}{cccccc}
    2& -1& 0 &\dots   & \dots &0 \\
    -1 & 2 & -1 &0 &\dots &\vdots \\
     0&-1 &2 & -1 & 0 & \vdots \\
     \vdots & \dots   & \dots &\dots   &\dots & \vdots \\
     0&\dots   &\dots  &-1 &2& -1 \\
     0&\dots    &\dots  & 0  &-1 & 2 \\
    \end{array} \right)    
    \left(\begin{array}{c}
    v{1}\\
    \vdots \\
     v_{i-1}\\
     v_{i}\\
     v_{i+1}\\
     \vdots \\
     v_{n}\\
    \end{array} \right)=
    h^{2}\left(\begin{array}{c}
    f{1}\\
    \vdots \\
     f_{i-1}\\
     f_{i}\\
     f_{i+1}\\
     \vdots \\
     f_{n}\\
    \end{array} \right)$$\\
    
$$\Downarrow$$
$$2v_{1}-v_{2} = h^{2}f_{1}$$
$$-v_{1}+2v_{2}-v_{3} = h^{2}f_{2}$$
$$-v_{2}+2v_{3}-v_{4} = h^{2}f_{3}$$
$$\vdots$$
$$-v_{i-1}+2v_{i}-v_{i+1} = h^{2}f_{i}$$
$$\Downarrow$$
$$\uuline{-\frac{v_{i+1}+v_{i-1}-2v_{i}}{h^{2}} = f_{i}}$$

\newpage
In the folloiwng we will assume that $f(x) = 100e^{-10x}$, and we will use the boundary conditions given in Equation (\ref{eq:boundary}). Then Equation (\ref{eq:2_diff_eq}) has the closed-form solution $u(x) = 1-(1-e^{-10})x-e^{-10x}$. To test if this is the right solution we take the derivative of $u''(x)$ twice.\\

$$u'(x) = -(1-e^{-10})x-e^{-10x}$$
$$\Downarrow$$
$$-u''(x) = 100e^{-10x} = f(x)$$\\
\vspace{5mm}
The analytical solution $u(x)$ is correct, and it can be used as a comparison to a numerical solution.\\
\vspace{5mm}

To find the numerical solution of Equation (\ref{eq:2_diff_eq}) one can use the method Gaussian elimination or LU-decomposition, which are discussed in the theory section. Before we use one of the two methods we have to take a good look at the matrix equation that has to be solved. In this case we have shown that the matrix $\bf A$ is a tridiagonal matrix, and we then realize that the problem can be solved in a more efficient way. A general tridiagonal matrix can be expressed on the form\\

 $${\bf A} = 
    \left(\begin{array}{cccccc}
    b_{1} & c_{1}& 0 &\dots   & \dots &0 \\
    a_{2} & b_{2} & c_{2} &0 &\dots &\vdots \\
     0& a_{3} & b_{3} & c_{3} & 0 & \vdots \\
     \vdots & \dots   & \dots &\dots   &\dots & \vdots \\
     0&\dots   &\dots  & a_{n-1} & b_{n-1} & c_{n-1} \\
     0&\dots    &\dots  & 0  & a_{n} & b_{n} \\
    \end{array} \right).$$\\
\vspace{5mm}

The set of equations derived from the matrix equation (\ref{eq:the_matrix_eq}) is then\\

\begin{equation}\label{eq:tri_d_eq}
a_{i}v_{i-1} + b_{i}v_{i} + c_{i}v_{i+1} = b_{i}.
\end{equation}
\vspace{5mm}

Now we can use the expression (\ref{eq:tri_d_eq}) to make an algorithm based on the formulas for forward substitution (\ref{eq:lhs_forw}) and (\ref{eq:rhs_forw}), and backward substitution (\ref{eq:backw}). This will end up to be a more efficient variant of the Gaussian elimination method, because we will ignore all the matrix elements that we know is zero.\\

\newpage
The update of the coefficients in the tridiagonal matrix then goes like\\

$$a_i = a_i - \frac{a_i\cdot b_i}{b_i} = 0,\quad b_i = b_i - \frac{a_{i-1}\cdot c_{i-1}}{b_{i-1}},\quad c_i = c_i - \frac{a_{i-1}\cdot 0}{b_{i-1}} = c_i,$$\\
\vspace{5mm}

and the update of the right hand side of the equation goes like\\

$$f_i = f_i - \frac{a_{i-1}\cdot f_{i-1}}{b_{i-1}}.$$\\

Then we can find the unknowns by backward substitution, and the algorithm in this case goes as\\

$$v_i = \frac{f_i - a_i\cdot v_{i-1} - c_i\cdot v_{i+1}}{b_{i-1}}$$\\
\vspace{5mm}

We can take the simplifications further by use of the fact that $a_i = c_i = 1$ (when the minus sign is factorized out), and that the factor $\frac{a_{i-1}}{b_{i-1}} = \frac{1}{b_{i-1}}$ is found in the formula for both $b_i$ and $f_i$.  In addition, all the elements $a_i = 0$ when we comes to the backward substitution, and so we get rid of the $a_i$-part in that algorithm. The code-snippet for forward substitution and backward substitution is listed in Listing \ref{lst:fwrd_bkwrd}. In the algorithm $v_i$ $(u[i])$ has been pushed one element ahead, the reason for that is that the vector $u$ in the code is two elements longer because of the boundary conditions.\\
\vspace{5mm}

In this project we will count floating point operations in the way that addition, subtraction, multiplication and division, counts as one floating point operation each. In addition to count floating point operations, the time it takes to execute the customized algorithm for forward and backward substitution, is compared with the time it takes to execute the same job with LU-decomposition. We use the functions ludcmp() and lubksb() from the library lib.cpp to calculate the numerical solution by LU-decomposition, and uses the package time.h to take the time of the executions. 

\begin{center}
  \lstset{%
    caption=Forward and backward substitution for the tridiagonal matrix (\ref{matrix:tridiag_deriv}).,
    basicstyle=\ttfamily\footnotesize\bfseries,
    frame=tb
  }
\begin{lstlisting}[label={lst:fwrd_bkwrd}]
for(i=1;i<=(n-1);i++)
{
	//forward substitution
	double factor = (1.0/b[i-1]);
	b[i] = b[i] - factor;
	y[i] = y[i] - (factor*y[i-1]);
	a[i-1] = 0;
}
for(i=n;i>=1;i--)
{
	//fill u(x) by backward substitution
	u[i] = (y[i-1]-u[i+1])/b[i-1];
}
\end{lstlisting}
\end{center}
\newpage
When the calculations by use of the algorithm in Listing \ref{lst:fwrd_bkwrd} is executed, the relative error of the numerical solution is found by use of Equation (\ref{eq:relative_error}). We will check the relative error for grid-points in the range $n=10$ to $n=10^6$, to find the $n$ that gives the least error. The error is of course getting smaller for bigger $n$, but at some point the step lengths are getting so small that round off errors in the calculations will appear.

\section*{Results and discussion}
\subsection*{Floating point operations and execution time}
For the method in Listing \ref{lst:fwrd_bkwrd} there is $6n$ floating point operations, where $n$ is the size of the quadratic matrix, and so also the number of unknown parameters. In Gaussian elimination there is needed $\frac{2}{3}n^{3}$ floating point operations to find the answer, and by use of LU-decomposition there is needed floating point operations in magnitude of $n^2$. Then we know that the customized algorithm for the case we are looking at, is the most efficient of those three.\\
\vspace{5mm}
We have also measured the time that the execution of the algorithm in Listing \ref{lst:fwrd_bkwrd} and the LU-decomposition takes. The results is listed in Table \ref{tab:time}.
\vspace{5mm}
\begin{table}[!h]
\begin{center}
\begin{tabular}{| c | c | c |}
	\hline
	\textbf{Log10(h)}  & \textbf{Algorithm Listing (\ref{lst:fwrd_bkwrd}) [sec]} &  \textbf{LU-decomposition [sec]}\\
	\hline		
	$-1$ & $3.0\cdot 10^{-6}$ & $8.4\cdot 10^{-5}$ \\
    $-2$ & $4.0\cdot 10^{-6}$ & $1.6\cdot 10^{-3}$\\
    $-3$ & $2.7\cdot 10^{-5}$ & $2.7$\\
  \hline
\end{tabular}
\end{center}
\caption{\label{tab:time}Execution time for the algorithm in Listing \ref{lst:fwrd_bkwrd} and LU-decomposition.}
\end{table}
\vspace{5mm}

Here we can see the result of the difference in floating point operations for the two methods. LU-decomposition is using much more time than the customized algorithm, and the time used is growing much faster for that method, as the grid-points in the calculation is increasing. Since the error for LU-decomposition is of the order $O(n^2)$ one can assume that the execution time is increasing as a parabola with $n$. Because of that the execution time for the LU-decomposition with $h=-4$ ($n=10^4$) is going to be inconveniently long, and for $h=-5$ ($n=10^5$) the algorithm is more or less useless.  

\newpage
\subsection*{Numerical solution}
The numerical solution from the customized algorithm is plotted in the figures \ref{fig:num_10}, \ref{fig:num_100} and \ref{fig:num_1000}.
\vspace{5mm}
\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{num_10.png}
\caption{\label{fig:num_10}Plot of numerical solution together with the analytically solution for n = 10.}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{.5\textwidth}
 		\centering
 		\includegraphics[width=1\linewidth]{num_100.png}
  		\caption{Numerical and analytically solution}
  		\label{fig:num_100_}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
  		\centering
  		\includegraphics[width=1\linewidth]{num_100_zoom.png}
  		\caption{Numerical and analytically solution zoomed}
  		\label{fig:num_100_zoom}
	\end{subfigure}
	\caption{Plot of numerical solution together with the analytically solution for n = 100.}
	\label{fig:num_100}
\end{figure}

\vspace{10mm}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{.5\textwidth}
 		\centering
 		\includegraphics[width=1\linewidth]{num_1000.png}
  		\caption{Numerical and analytically solution}
  		\label{fig:num_1000_}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
  		\centering
  		\includegraphics[width=1\linewidth]{num_1000_zoom.png}
  		\caption{Numerical and analytically solution zoomed}
  		\label{fig:num_1000_zoom}
	\end{subfigure}
	\caption{Plot of numerical solution together with the analytically solution for n = 1000.}
	\label{fig:num_1000}
\end{figure}

\newpage
We can see that the numerical solution is getting drastically closer to the analytically solution when the grid points increases from $n=10$ to $n=100$. When the grid points is increased further, from $n=100$ to $n = 1000$, the numerical solution is getting even closer to the analytically solution, but not as drastically as the previous change.


\subsection*{Relative error}
At last the relative error of the numerical calculations is found for different step lengths. You can see the results in Table \ref{tab:rel_error}. The plot of the relative error is in Figure \ref{fig:rel_error}. We can see that the relative error is dropping linearly with slope approximately equal to $-2$, as the step length is dropping. In the area where the step length is around $h = 10^{-5}$, we can see that the relative error is getting unstable, and it is here the round off errors are taking over. The result tells us that the step length $h=10^{-5}$ is the most optimal step length if you want the least relative error, here the balance between the number of grid-points and the risk of round off errors is the best. 
\begin{table}[!h]
\begin{center}
\begin{tabular}{| c | c |}
	\hline		
  \textbf{log10(h)} & \textbf{log10(Error)} \\
  \hline	
  $-1$ & $-1.1796978$ \\
  $-2$ & $-3.0880368$\\
  $-3$ & $-5.0800516$\\
  $-4$ & $-7.0793568$\\
  $-5$ & $-9.0048965$\\
  $-6$ & $-6.7713740$\\
  \hline
\end{tabular}
\end{center}
\caption{\label{tab:rel_error}Relative error of numerically calculated solution.}
\end{table}
\begin{center}
\begin{figure}[!h]
\includegraphics[scale=0.7]{relative_error.png}
\caption{\label{fig:rel_error}Plot of maximum relative error for the numerical solutions, found by use of the algorithm in Listing \ref{lst:fwrd_bkwrd}, for different numbers of grid-points (step lengths).}
\end{figure}
\end{center}
\end{flushleft}
\end{document}









